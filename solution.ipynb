{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from src.solution_data_utils import split_data, clean_text\n",
    "from src.solution_evaluates import evaluate_rouge, evaluate_model, evaluate_rouge_gpt\n",
    "from src.solution_datasets import PredictWordDataset, ValuateDataset\n",
    "from src.solution_model import Predictor\n",
    "from src.solution_savepoints import save_to_file\n",
    "from src.solution_generates import generate\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Читаю конфигурацию",
   "id": "af69bcde74b8a897"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "print(config)\n",
    "texts = []\n",
    "model_name = config['model_name']"
   ],
   "id": "1ce0d12e7b254db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Читаю сырые сообщения, чищу и сохраняю в файл",
   "id": "238432f2a119f25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with open(\"data/tweets.txt\") as f:\n",
    "#     while s:=f.readline():\n",
    "#         text = clean_text(s) +'\\n'\n",
    "#         texts.append(text)\n",
    "# with open('data/cleaned_tweets.txt', \"w\") as f:\n",
    "#         f.writelines(texts)\n",
    "# print(texts[:5])"
   ],
   "id": "c354797a86c3bcac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Открываю очищенные сообщения, разбиваю на трейн, тест и валидейт",
   "id": "4cb39c99576f31d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('data/cleaned_tweets.txt') as f:\n",
    "    texts = f.read().splitlines()\n",
    "texts= list(filter(lambda x: len(x.split()) >=4, texts))\n",
    "if config[\"text_crop\"] >  0:\n",
    "    texts = texts[:config[\"text_crop\"]]\n",
    "print(texts[:5])\n",
    "\n",
    "train_texts, test_texts = split_data(texts, train_size=0.8, test_size=0.2)\n",
    "max_len = max((len(text) for text in train_texts))\n",
    "print(\"max_len=\", max_len)\n",
    "\n",
    "val_texts, test_texts = split_data(test_texts, train_size=0.5, test_size=0.5)\n",
    "print(\"len(train_texts)=\", len(train_texts), \" len(test_texts)=\", len(test_texts))\n"
   ],
   "id": "eda80478a7218666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загружаю токенайзер",
   "id": "337ef142b1c28309"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"tokenizer.eos_token=\", tokenizer.eos_token)\n",
    "print(\"AutoTokenizer.vocab_size\", tokenizer.vocab_size)"
   ],
   "id": "b1c08393058f5088",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создаю датасеты и даталоадеры",
   "id": "b68020f606c440da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_ds = PredictWordDataset(train_texts, tokenizer)\n",
    "val_ds = PredictWordDataset(val_texts, tokenizer)\n",
    "test_ds = ValuateDataset(test_texts)\n",
    "print(len(train_texts), \"->\", len(train_ds))\n",
    "\n",
    "print(len(train_texts), len(test_texts))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config.get('batch_size', 64), shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config.get('batch_size', 64))\n",
    "test_loader = DataLoader(test_ds, batch_size=config.get('batch_size', 64))"
   ],
   "id": "ded60fbb88f90699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создаю экземпляр lstm модели",
   "id": "15fd82b90e6389a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_conf = config['model']\n",
    "print(\"model_conf=\", model_conf)\n",
    "model = Predictor(vocab_size=tokenizer.vocab_size, embedding_dim=model_conf[\"embedding_dim\"],\n",
    "                  hidden_dim=model_conf[\"hidden_dim\"], n_layers=model_conf[\"n_layers\"],\n",
    "                  dropout=model_conf[\"dropout\"], device=device)\n",
    "model.to(device)\n",
    "parameters = sum(p.numel() for p in model.parameters())\n",
    "print(\"parameters=\", parameters)"
   ],
   "id": "9b12fa74827ff7d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создаю экземпляры оптимайзера, критерия",
   "id": "d6cb8d3e07b912b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_conf[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "id": "4b72d778162b96ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Основной цикл обучения",
   "id": "754cff8dbdd06fc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Основной цикл обучения\n",
    "n_epochs = config['n_epochs']\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "    save_to_file(model=model, optimizer=optimizer, epoch=epoch, loss=val_loss)\n",
    "\n",
    "\n",
    "    eval_res = evaluate_rouge(model, test_loader, tokenizer,rouge)\n",
    "    print(f\"epoch {epoch + 1} rouge=\", eval_res)\n",
    "\n",
    "    generated_line = generate(model, \"this is first my next word prediction\", tokenizer, max_len=10, device=device)\n",
    "    print(\"generated_line=\", generated_line)"
   ],
   "id": "9e60b0a029ec37b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загружаю предобученную модель и оцениваю по rouge",
   "id": "d080d8f4bda616e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tr_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# tr_model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=tr_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "prompt = \"this is first my next word prediction\"\n",
    "result = generator(\n",
    "    prompt,\n",
    "    # max_length=80,       # итоговая длина (включая prompt)\n",
    "    max_new_tokens=10,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,      # стохастическая генерация\n",
    "    top_p=0.95,          # nucleus sampling\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "print(\"generated_line\", result)\n",
    "res = evaluate_rouge_gpt(generator, test_loader, rouge)\n",
    "print(f\"rouge {model_name}=\", res)\n"
   ],
   "id": "bb36d39d07876db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Выводы:\n",
    "1. Модель\n",
    "2. Предобученная модель медленнее, но отвечает более правильно, не сточки зрения датасета,  а с общечеловеческой. Улучшить можно заменив последний слой модели и обучив на данной выборке"
   ],
   "id": "fbaa84e9a9b12a20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "19be2f351f3ed1d4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
